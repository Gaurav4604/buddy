
            <title>
            3.3   BASENES
3.3. BASELINES
            </title>
            

            <text>
            We compare our Col agent with recent works on idea generation and experiment design. To ensure a fair comparison, we employ GPT-40 and Semantic Search as the LLM and academic retriever implementations, respectively, across all baseline methods. Furthermore, we unify the output format of the generated ideas and experiment designs to minimize evaluation preference towards more structured outputs (Chiang et al., 2024). We compare with the following baselines:
            </text>
            

            <text>
            * **RAG**: This is a vanilla retrieval augmented generation approach (Lewis et al., 2020), where we directly prompt the LLM with retrieved literature for idea generation and experiment design.

* **ResearchAgent** (Baek et al., 2024): This work leverages additional academic knowledge graph for enhancing the literature retrieval and adopts a multi-agent framework to iteratively refine ideas through peer discussions. We follow the original paper to reproduce this baseline.

* **GPT-Researcher** (Assafelovic, 2023): GPT-Researcher is an agent framework specifically designed for the research domain. The agent is enhanced with plan-and-solve and RAG capabilities.

* **Al-Scientist** (2024): This work originally aims to generate the entire paper with the idea, methods, and experimental results. We extract the components related to idea generation and experiment design to serve as our baseline.

* **Real Paper**: Note that, in Sec. [3.2] we extract topics from existing research papers. Therefore, the ideas and the experimental designs from these papers serve as a natural baseline to quantify the gap between model-generated ideas and genuine human ideas.
            </text>
            

            <title>
            3.4   EvalULATION: IDEA ARENA
3.4 EVALUATION: IDEA ARENA
            </title>
            

            <text>
            **Model-based Evaluation.** The open-ended nature of idea generation poses challenges for automatic evaluation. Prior work primarily uses LLM-based Likert scale system to score ideas (2024). However, show this method poorly aligns with human preferences. Instead, they show LLMs perform better in ranking ideas. To obtain reliable scores for evaluation, we propose Idea Arena, a pairwise evaluation system using a Round-Robin tournament to compute ELO scores for each idea-generation method. For a given topic, we require the LLM judge to rank the ideas generated by any pair of methods (Table [24). We evaluate each pair twice with order reversed to reduce the position bias. To comprehensively evaluate an idea from multiple perspectives, we incorporate criteria from ICML 2020 review guidelines (5 and those in (2024), which consist of Novelty, Significance, Clarity, Feasibility, and Expected Effectiveness. Finally, the resultant win-loss-tie records are utilized to calculate the ELO scores for each method,

following the practices outlined in [Zheng et al.| (2024) ; Zhao et al.| (2024). We also evaluate the experiment design in the same pairwise way, focusing on Feasibility, Technical Quality, and Clarity.

Refer to Definitions for all metrics in Tables|5| [SJand|6|of the Appendix.
            </text>
            

            <text>
            **Human Evaluation.** We also perform human evaluation in a pairwise manner. The 10 researchers who review the extracted topics are asked to rank two ideas and experiment designs using the same criteria. To ensure fairness, we anonymize the source of the ideas by concealing the method identity.
            </text>
            

            <title>
            4   SULTS
4 RESULTS
            </title>
            

            <title>
            4.1   IDEA GENERATION

4.1 IDEA GENERATION
            </title>
            

            <text>
            **Main results.** Figures [3] and [4] present the results of idea generation evaluated by both a LLM (specifically, GPT-40) and human researchers. Detailed scores are in Table[26]of Appendix. Overall, our Col agent demonstrates superior performance compared to all other automated methods in both model-based and human-based evaluations. Notably, it substantially outperforms the second-best baselines, GPT-Researcher and RAG, by margins of 108 and 56 ELO scores, respectively, in the two evaluation settings. Our Col agent’s performance is on par with that of the Real Paper baseline and even excels in the metrics of Novelty and Significance. These results highlight its exceptional capabilities in idea generation. Furthermore, Col demonstrates superior performance in Clarity, Feasibility, and Expected Effectiveness compared to other automated methods in human evaluation. Nevertheless, it still lags considerably behind the Real Paper in these areas. This substantial gap

            </text>
            
