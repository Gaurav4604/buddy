
            <text>
            existing works, and the key entities extracted from existing literature, as described in Sec. [2.2](Tables [12] and [13]). These entities comprise relevant datasets and potential baseline models, which are important to clarify the concepts mentioned in the existing literature. After obtaining the future trend, we continue to prompt the LLM to articulate its motivation, novelty, and methodology, finally consolidate the idea (Tables14]and[15). Through this step-by-step manner, COI can produce a more detailed idea. Following the previous practice (2024), we also use a
novelty-check agent to evaluate candidate ideas. It retrieves relevant papers and prompts another
LLM to assess the similarity between the generated idea and the retrieved papers (Table[16). Based
on this assessment, our framework determines if another round of generation is necessary. Finally,
we pairwisely compare the generated ideas from all Col branches and select the one with the highest
winning rate as the final idea for the experiment design. This pairwise comparison follows the same
method as Idea Arena, refer to Sec. [3.4] for details.
            </text>
            

            <title>
            2.4 EXPERIMENT DESIGN
            </title>
            

            <text>
            While our primary goal is to generate novel ideas, it is also useful to develop experimental plans that help users implement these ideas. Thus, we extended the CoI agent to include experiment design. As shown in the lower-right of Figure[2], we prompt the LLM with experiments from existing works obtained from Sec. [22] as few-shot examples, along with the proposed idea and key entities, to guide the LLM in designing experiments for our ideas (Table[1]).
            </text>
            

            <text>
            We also employ a review agent to assess the candidate experiment designs. Its main role is to evaluate the clarity and comprehensiveness of the protocol, ensuring all key elements—such as datasets and models—are clearly specified. Additionally, it checks if the design provides enough detail for practical implementation (Table [18). The review agent provides critical feedback on these aspects, subsequently utilizing this information to conduct further searches for relevant literature (Table [19) to help the LLM refine and enhance its previous experiment design (Table (20). Through this iterative process of review and refinement, we arrive at a final experiment design.
            </text>
            

            <title>
            3. EXPERIMENTAL SETUPS
            </title>
            

            <title>
            3.1   IMPLEMENTATIONS

3.1 IMPLEMENTATIONS
            </title>
            

            <text>
            In our Col agent, we primarily use GPT-40 (05-13) as our LLM implementation. For some modules that require full-paper understanding, we use GPT-40-mini (07-18) to read the paper and summarize the core contents due to its lower price and good summarization capability. We use Semantic Scholar as our academic search engine. For the main experimental results, the maximum length of the Col is set to 5 and the number of Col branches is set to 3, and their analysis results are given later. The iteration number of self-refinement in the experiment design stage is set to 1 for cost saving.
            </text>
            

            <title>
            3.2 DATA

            </title>
            

            <text>
            To evaluate our Col agent’s ability to generate novel ideas, we collect recent research topics from Hugging Face’s Daily Paper| known for its timely updates on AI research and the high quality of the featured papers. We select papers submitted between August 1 and September 15, 2024, ensuring that the topics are sufficiently new and the time frame is after the data cutoff of the LLM. We ask 10 skilled researchers with diverse interests in AI to identify papers that capture their interests. Subsequently, we prompt GPT-4o0 to extract research topics, proposed ideas, and their corresponding experiment designs from these selected papers (Tables Table [22] and [23). The extracted topics will then be returned to the researchers for validation, ensuring that the extracted topics are valid and reasonable within their research domains. The extracted ideas and experiment designs will be utilized as our Real Paper baseline, as described in Section [3.3]. Due to the substantial costs associated with generating and evaluating ideas and experiment designs, we adhere to the assessment scale of|Lu et al.|(2024); (2023) to collect 50 research topics in total for evaluation.
            </text>
            

            <formula>
            https://openai.com/api//https://huggingface.co/papers/
            </formula>
            
