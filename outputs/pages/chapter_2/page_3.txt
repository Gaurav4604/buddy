
        <image>
        <path>
        outputs/images/chapter_2/page_2_0.jpg
        </path>
        <description>
        The diagram illustrates a process flow for enhancing large language model problem-solving capability. It is divided into three stages: CoI Construction, Idea Generation, and Experiment Design.
        </description>
        </image>
        

        <image>
        <path>
        outputs/images/chapter_2/page_2_1.jpg
        </path>
        <description>
        A diagram illustrating a process for enhancing large language model problem-solving capability. It is divided into three stages: CoI Construction, Idea Generation, and Experiment Design.
        </description>
        </image>
        

            <caption>
            # Figure 2: Our proposed CoI agent framework.
            </caption>
            

            <text>
            anchor. We then prompt the constructed Col to an LLM for idea generation and experiment design. During idea generation, we require the LLM to predict possible future trends. This prognostic result facilitates the gradual consolidation of the idea, beginning with the motivation for the proposed idea, progressing through an assessment of its potential impact, and culminating in the realization. However, as the evolution of scientific discovery can emerge from multiple perspectives, a single Col may be insufficient to capture the most promising direction. Additionally, there is no guarantee that the generated ideas will be novel. To address these issues, we construct multiple Col branches for different perspectives of a research topic. Additionally, a novelty-checker agent iteratively evaluates the draft idea against existing literature and refines it if substantial similarity is identified.
            </text>
            

            <text>
            We compare our Col agent against existing baselines on idea generation in the artificial intelligence (AI) field. To do this, we develop an arena-style evaluation framework called Idea Arena where participant methods compete in pairs, which demonstrates high agreement with human evaluation. The experimental results show that Col agent consistently ranks first among all automated baselines, surpassing the second-best one by 56 ELO scores in human evaluation. Col agent can generate ideas as novel as those of human experts. Our analysis further shows that for LLMs to generate novel ideas, a clear developmental trend analysis is more pivotal than the quantity of related literature.
            </text>
            

            <text>
            Our contributions are summarized as follows: 1) We propose the Col agent to enhance LLMs' capability in idea generation. Col agent organizes relevant literature in a chain structure to effectively mirror the progressive nature of research development, allowing LLMs to better grasp the current research advancements. 2) We propose Idea Arena for a comprehensive evaluation of idea-generation methods, which shows high agreement with human researchers. 3) Extensive experiments demonstrate the effectiveness of our Col agent in generating ideas that are comparable to human creativity.
            </text>
            

            <title>
            2  METHOD

2 METHOD
            </title>
            

            <title>
            2.1 FraM E W o R K: C H A i N-O F-I D E A S A G E N T
            </title>
            

            <text>
            In this section, we detail our Col agent framework, as illustrated in Figure [2] which consists of three stages: (1) CoI Construction, (2) Idea Generation, and (3) Experiment Design. First, given a research topic, the Col agent constructs multiple Cols from existing literature, reflecting different trends within the domain. Then, for each Col, the LLM predicts future research directions, and crafts ideas through step-by-step consolidation and iterative novelty checks. The best idea is then selected. Lastly, the LLM generates and refines an experiment design to implement the final idea.
            </text>
            
