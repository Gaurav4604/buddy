
            <title>
            

**TOTEM: TQkenized Time Series EMbeddings for General Time Series Analysis**
            </title>
            

            <text>
            Sabera Talukder\ Yisong Yue \ Georgia Gkioxari \ California Institute of Technology
            </text>
            

            <text>
            saberah@caltech.edu
yuyu@caltech.edu
georgia@caltech.edu
            </text>
            

            <text>
            Reviewed on OpenReview: https://openreview.net/forum?id=QLILkH6eRG
            </text>
            

            <title>
            Abstract
            </title>
            

            <text>
            This work studies the problem of time series analysis with *generalist* (or foundation) models, which are models trained across many data domains. Drawing inspiration from the widespread success of large language models, we consider the simple strategy of discretely tokenizing time series data drawn from a myriad of datasets via self-supervision, then using the fixed tokenization to solve a variety of tasks across many data domains. Canonically, time series to estimate the probability of the set of all the agents in the model. As such, forcasting-only model), where many use patches of time as inputs to the model. As such, performant generalist, discrete representation time series models explored across many tasks are of value. Our method, Tokened Time Series Edmonds (TOTEM), produces a well-open list from exercise. We use the TOTEM or not given-out rule by exploiting some three component-studied time series tasks with real-world data: imputation (17 baselines, 12 datasets), anomaly detection (19 baselines, 25 datasets), and forecasts (14 batches, 12 datasets). We conclude that TOTEM matches or outperforms existing state-of-the-art models in both the canonical specialist setting (i.e., training one model on one domain), as well as the generalist section (i.e., information one may sometimes use one or more), which demonstrates the fact that the (location for a real time series analysis. The one-source implementation is available here: [https://github.com/SaberraTalkley/TOTEM](https://www.youtube.com/watch?v=OqrCpdb6MkJ
            </text>
            

            <title>
            
    1.  Introduction

            </title>
            

            <text>
            Time series are a fundamental data modality, generalizing large classes of time-varying data from many domains, like weather phenomena, electrical grid activity, or traffic flow. Most commonly, time series analysis is first restricted to one such domain, then to a specific task, like imputation (Luo et al., 2019; Talukder et al., 2022), anomaly detection (Xu et al., 2021; He & Zhao, 2019), or forecasting (Wu et al., 2021)}, among others. Though these domains and tasks are quite distinct, a natural question is whether it is possible to design domain-agnostic models adaptable to any task. This question is the subject of our work.
            </text>
            

            <text>
            Generalist models are those trained on many data domains simultaneously (e.g., weather, electricity, traffic, etc.), while specialist models are those trained on a single time series domain (e.g., weather only), as shown in Figure [1] (2022). Both generalist and specialist models can be tested in two ways: in-domain testing, where a model is tested on the same domain(s) on which it was trained, and zero-shot testing, where it is tested on different domain(s) (see Figure fiB). Performing zero-shot testing on specialist models is not uncommon. For example, some works have studied zero-shot forecasting, where a forecaster trains on one dataset then predicts on a separate dataset (Zhou et al.||2023), or trains on a subset of channels (which we call sensors) from one dataset then forecasts zero-shot on the remaining sensors in the
            </text>
            
