
            <text>
            No Domain-specific Data Engineering. Many prior works (especially in time series forecasting) leverage domain-specific knowledge to landmark features that encode critical information. For instance, works that study calendar-based time series of data auxiliary features that denote landmarks like the first day of a month or holidays (Given et al. [2023]) **Samsin = 21** [2020]**. Other works properly-engineered architectures that convert time series into frequency space representations. For example, TimeNet operates around the time series of frequency space and frequency-space imaging by computing the Fourier transform on several subsets of the time series (Wiu et al. [2022]). Similarly, ForEorder represents a time series with a random subset of its Fourier components and modulation (ReIN) (Kint et al. [2024]) to represent temporal waveforms in a normalized space (see Figure [20]), which requires no assumptions on the form of the data. This allows TOTEM to generalize across domains and outperform the prior handled methods on many distinct tasks using simple, generic architectures.


            </text>
            

            <title>
            3.3. Tokenizer Implementation
            </title>
            

        <image>
        <path>
        outputs/time_series/images/chapter_0/page_4_2.jpg
        </path>
        <description>
        <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow>
    <mspace width="20px" height="45px" form="stretch" stretch-to="right" stretch-type="increase"></mspace>

   <mo>+</mo>

   <mspace width="16px" height="37px" form="stretch" stretch-to="right" stretch-type="increase"></mspace>

    <mrow>
      <mi>S</mi>
      <mn>x</mn>
      <mi>E</mi>
     </mrow>
   
  </mrow></math>
        </description>
        </image>
        

            <text>
            Though TOTEM is a VQVAE, the design of the encoder and decoder differ substantially from the original model and similar works like WaveNet, which use dilated convolutions (I. Dord et al., 2017). The dilations in these architectures skip many time steps, allowing the convolutional filters to operate on a larger input area at a coarser scale, improving model efficiency. However, this design decision is a normalized space.

Motivated by the high sampling rates of digital audio waveforms,
which is not a universal trait across time series domains (see Table 6). In contrast, TOTEM uses a stack of strided 1D convolutions with a dilation of 1 such that it can account for every time step. Using a long input (e.g., 96 time steps for standard forecasting tasks) allows TOTEM to maintain a large receptive field. Lastly,
the use of RevIN allows TOTEM to remain effective by only learning a small set of normalized waveforms, and if the unnormalized reconstruction is required for a downstream task, the normalization parameters can also be passed to the decoder (see Figure 4).

Formally, TOTEM accepts a batch of univariate time series {x; R^T} obtained by flattening the sensor channel of the multivariate data. An encoder E consisting of a stack of strided 1D convolutions then temporally compresses the data by a total factor of F to recover a latent variable z = E(x) in R^{FxD}, where D is the latent feature dimension. The latent variable z is then quantized into an element Î² of the codebook C = {c}*, consisting of k D-dimensional codewords c; R^D following the relation Î² = ce, where Î± = arg min; ||z - c||_3. The decoder D mirrors the encoderâ€™s architecture, mapping the quantized embedding Î² to a reconstructed time series X = D(z) in R^{FxD}.
            </text>
            

            <text>
            As in [Van Den Oord et al.] (2017), we train â„°, ğ’Ÿ, and ğ’ by optimizing the objective . 
            </text>
            

            <formula>
            
    â„’=1/EÂ· Sâˆ‘_i||ğ±_i-ğ±Ì‚_i||_2^2_â„’_rec+||ğ¬ ğ [ğ³]-ğ³Ì‚||_2^2_â„’_vq+Î²||ğ³-ğ¬ ğ [ğ³Ì‚]||_2^2_â„’_cmt

            </formula>
            

            <text>
            
    (ğ¥)

            </text>
            

            <text>
            where ğ¬ğ [Â·] is the stop-gradient operator and Î² is the commitment loss weight. For additional details, see Appendices [11] and [12]. In all experiments, we use a compression factor of F=4, (see Table [22]).


            </text>
            

            <title>
            

## 3.4 Forecasting Model Implementation
            </title>
            

            <text>
            In contrast with prior works, **TOTEM** is capable of solving the imputation and anomaly detection tasks **with the tokenizer alone (see Figures [1] and [2]).** Therefore, the only downstream model we must design is the forecasting model. First, each sensor's observations ğ±_iâˆˆâ„^7n are converted into a sequence of ğ•‹_n/F discrete tokens ğ±Ì‚_i. The forecasted process adds temporal positional embeddings to these tokens, passing them through a transformer encoder consisting of a series of multi-head attention layers that attend along the time dimension to predict normalized measurements ğ²Ì‚_iâˆˆâ„^7_test for i=1,...,S.


            </text>
            
