
        <image>
        <path>
        outputs/time_series/images/chapter_0/page_8_0.jpg
        </path>
        <description>
        A large image showing various charts and graphs related to performance metrics for different models. The chart on the left shows a comparison between two methods, Totem and GPT2, with their respective scores in terms of accuracy (Acc) and mean absolute error (MAE). Another section compares generalist zero-shot In-Domain models like TOTEM, GPT2, and an unspecified model labeled 
        </description>
        </image>
        

            <text>
            
# B. Generalist In-Domain


    



    

            </text>
            

            <text>
            
# C. Generalist Zero-Shot


    



    

            </text>
            

            <text>
            Table 5: Imputation Summary. In all categories TOTEM has SOTA AvgWins . In the specialist TOTEM has 52.1% AvgWins ; in generalist in domain TOTEM has 58.3%; in generalist zero shot TOTEM has 80.0%.
            </text>
            

            <text>
            Since we only use 3 seeds, we run a non-parametric *permutation test* on the generalist models in Appendix A.6 to analyze the performance of TOTEM vs. GPT2 (Table[24]), and TOTEM vs. PatchTOTEM (Table [25]). We find that TOTEM statistically significantly (pâ‰¤0.05) outperforms GPT2 in terms of Avg2ins on all tasks for both the in-domain and zero-shot testing programs. Additionally, TOTEM outperforms PatchTOTEM in a statistically significant (pâ‰¤0.05) manner for in-domain and zero-shot testing.

            </text>
            

            <title>
            
    5.1  Imp u t a t i o n

            </title>
            

            <text>
            In imputation, models intake a masked time series ğ±_ğ¦âˆˆâ„^SÃ— T_m, and then inpute the signal ğ±Ì‚âˆˆâ„^SÃ— T_m (see Figure [1]). We experiment with four canonical masking percentages at 12.5
            </text>
            

            <text>
            
ğ’ğ©ğğœğ¢ğšğ¥ğ¢ğ¬ğ­ğ¬. Figure[fig](a) and Table[fig](b) compare TOTEM to specialist baselines. All models are trained and evaluated on the same dataset (in-domain). TOTEM has the highest Agyâˆ•Hs with 52.1
            </text>
            
