
            <title>
            
# EALM: Retrieval-Augmented Language Model Pre-Training


    



    

            </title>
            

            <text>
            Kelvin Guu, Kenton Lee, Zora Tung, Pampong Pasupat, Ming-Wei Chang
            </text>
            

            <title>
            Abstract
            </title>
            

            <text>
            Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts.


            </text>
            

            <text>
            To capture knowledge in a more modular and interpretable way, we augment language model pretraining with a latent _knowledge retriever_, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents.


            </text>
            

            <text>
            We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16


            </text>
            

            <title>
            1. Introduction
            </title>
            

            <text>
            Recent advances in language model pre-training have shown that models such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2019)
            </text>
            

            <text>
            Equal contribution Google Research. Correspondence to: Kelvin Guu <kguu@google.com>, Kenton Lee <kentonl@google.com>, Zora Tung <gatoatigrado@google.com>, Panupong Pasupat <ppasupat @ google.com>, Ming-Wei Chang <mingweichang @ google.com>.
            </text>
            

            <text>
            _Proceedings of the 37^th International Conference on Machine Learning_, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s).


            </text>
            

        <image>
        <path>
        outputs/general/images/chapter_0/page_0_10.jpg
        </path>
        <description>
        The diagram illustrates a process flow for generating an answer from textual knowledge corpus and query documentation. It starts with unlabeled text, which is pre-trained on a corpus (X). The [MASK] at the top of the pyramid indicates that less material higher up in the pyramid can be used to retrieve relevant documents or information using a neural knowledge retriever model pŒ∏(z|x) . These retrieved documents are then processed by a knowledge-augmented encoder, which outputs an answer based on the query and document context. The process is iterative, with backpropagation from end-to-end.
        </description>
        </image>
        

            <caption>
            _Figure 1._ REALM augments language model pre-training with a **neutral knowledge retriever** that retrieves knowledge from a **textual knowledge corpus**, ùíµ (e.g., all of Wikipedia). Signal from the language modeling objective backpropagates all the way through the retriever, which must consider millions of documents in ùíµ‚Äìa significant computational challenge that we address.


            </caption>
            

            <text>
            store a surprising amount of world knowledge, acquired from the massive text corpora they are trained on (Petroni et al., 2019). For example, BERT is able to correctly predict the missing word in the following sentence: ‚ÄúThe pound‚Äù is the currency of the United Kingdom‚Äô (answer: ‚Äúpound‚Äù).
            </text>
            

            <text>
            In these language models, the learned world knowledge is stored _implicitly_ in the parameters of the underlying neural network. This makes it difficult to determine what knowledge is stored in the network and where. Furthermore, storage space is limited by the size of the network‚Äìto capture more world knowledge, one must train ever-larger networks, which can be prohibitively slow or expensive.


            </text>
            

            <text>
            To capture knowledge in a more interpretable and modular way, we propose a novel framework, Retrieval-Augmented Language Model (REALM) pre-training, which augments
            </text>
            
