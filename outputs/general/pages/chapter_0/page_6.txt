
            <title>
            

## 4.1. Open-QA Benchmarks
            </title>
            

            <text>
            A number of benchmarks have been proposed for Open-QA. In this work, we focus on datasets where the question writers did not already know the answer. This yields questions that reflect more realistic information-seeking needs, and also avoids artifacts that can arise if the question is formulated with a particular answer in mind. A deeper justification is given in Lee et al. (2019). In all cases, the predicted answer is evaluated via exact match with any reference answer, following previous Open-QA work (Chen et al., 2017).


            </text>
            

            <text>
            NaturalQuestions-OpenThe NaturalQuestions dataset (Kwiatkowski et al., 2019) consists of naturally occurring Google queries and their answers. Each answer also comes with an "answer type": following Lee et al. (2019), we only keep questions that are categorized as "short answer type" with at most five tokens. The dataset also provides a suggested Wikipedia document to retrieve; like all models we compare against, we do not provide this to our model.


            </text>
            

            <text>
            

**WebQuestions** The WebQuestions dataset (Bertant et al., 2013) was collected from the Google Suggest API, using one seed question and expanding the set to related questions. We follow the setting defined by Chen et al. (2017).


            </text>
            

            <text>
            

**CuratedTree** The CuratedTree dataset is a collection of question-answer pairs drawn from real user queries issued on sites such as MSNSearch and AskJeves. To account for multiple correct answers or different spelling variations, the answers in this dataset are defined as regular expressions that match all correct answers. It is unclear how to train generation-based models with this type of supervision, so we do not evaluate them on this dataset.


            </text>
            

            <title>
            

## 4.2. Aproaches compared
            </title>
            

            <text>
            Retrieval-based Open-QA Most existing Open-QA systems answer the input question by first retrieving potentially relevant documents from a knowledge corpus, and then using a reading comprehension system to extract an answer from the documents. In this paradigm, the knowledge is stored explicitly in the corpus. We wish to compare different methods for implementing retrieval.
            </text>
            

            <text>
            Many approaches use non-learned heuristic retrieval such as sparse bag-of-words matching (Robertson et al., 2009) or entity linking on the question to select a small set of relevant documents (e.g., 20). These documents are typically then re-ranked using a learned model, but coverage may be limited by the initial heuristic retrieval step. Approaches such as DrQA (Chen et al., 2017), HardEM (Min et al., 2019a), GraphRetriever (Min et al., 2019b), and PathRetriever (Asai et al., 2019) in Table 1 are in this category.


            </text>
            

            <text>
            Some recent approaches have proposed to implement learning.
            </text>
            

            <text>
            able retrieval using a MIPS index. ORQA (Lee et al., 2019) formulates Open-QA using a similar latent variable model as REALLY, and also trains by maximizing the marginal likelihood. However, REALLY adds a novel language model pre-training step, and backpropagates into the MIPS index, rather than using a fixed index. In Table 1, we directly compare the two. It is also important to note that the retrievals for both REALLY pretraining and ORQA are initialized using the Inverse Cloze Task, described in Section 3.4.


            </text>
            

            <text>
            Generation-based Open-QA An emerging alternative approach to Open-QA is to model it as a sequence prediction task: simply encode the question, and then decode the answer token-by-token based on the encoding. While it was initially unclear how large amounts of knowledge could be injected into the model, GPT-2 (Radford et al., 2019) hinted at the possibility of directly generating answers without using any given context via sequence-to-sequence. However, their performance was not competitive possibly due to the lack of fine-tuning. Orthogonally, TS (Raffel et al., 2019) showed that directly generating answers without explicit extraction from the given context is viable approach, but they only experimented on the reading comprehension task, where a context document is provided.


            </text>
            

            <text>
            For the most competitive and comparable generation-based baseline, we compare to concurrent work which fine-tumes T5 for Open-QA (Roberts et al., 2020).[4] We compare against the Base, Large, and even larger 11-billion parameter model to measure the effect of model size.


            </text>
            

            <title>
            

## 4.3. Implementation Details


            </title>
            

            <text>
            

**Pre-training** We pre-train for 200k steps on 64 Google Cloud TPUs, with a batch size of 512 and a learning rate of 3e-5, using BERT's default optimizer. The document embedding step for the MIPS index is parallelized over 16 TPUs. For each example, we retrieve and marginalize over 8 candidate documents, including the null document ‚àÖ.


            </text>
            

            <text>
            We experiment with two choices of the pre-training corpus ùí≥: (1) Wikipedia, which is identical to the knowledge corpus ùíµ, and (2) CC-News, our reproduction of the corpus of English news proposed by Liu et al. (2019).

            </text>
            

            <text>
            

**Fine-tuning** We follow the ORQA (Lee et al., 2019) fine-tuning approach but initialized with the pre-trained REALM components. The knowledge corpus is derived from the December 20, 2018 snapshot of English Wikipedia. Documents are greedily split into chunks of up to 288 BERT wordpieces, resulting in just over 13 million retrieval candidates.


            </text>
            

            <text>
            We initially conducted our own T5 experiments using the code from https://tinyurl.com/t5-open-ag-colab (Raffel et al., 2019). We now report results from the concurrent work of Roberts et al. (2020), which has an improved fine-tuning procedure.
            </text>
            
