
            <text>
            Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,
Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V.
Roberta: A robustly optimized bert pretraining approach.
arXiv preprint arXiv: 1907.11692, 2019.

Mikolov, T., Chen, K., Corrado, G., and Dean, J. Efficient
estimation of word representations in vector space. arXiv
preprint arXiv: 1301.3781, 2013a.

Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and
Dean, J. Distributed representations of words and phrases
and their compositionality. In Advances in neural information
processing systems, pp. 3111-3119, 2013b.

Miller, A., Fisch, A., Dodge, J., Karimi, A.-H., Bordes, A,
and Weston, J. Key-value memory networks for directly reading documents.
arXiv preprint arXiv: 1606.03 126,
2016.

Min, S., Chen, D., Hajishirzi, H., and Zettlemoyer, L. A
discrete hard em approach for weakly supervised question answering.
arXiv preprint arXiv: 1909.04849, 2019a.

Min, S., Chen, D., Zettlkemoyer, L., and Hajishirzi,
H. Knowledge guided text retrieval and reading for open domain question answering.
arXiv preprint arXiv:1911.03868, 2019b.

Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,
C., Lee, K., and Zettlemoyer, L. Deep contextualized word representations.
In Proc. of NAACL, 2018.

Peters, M. E., Neumann, M., IV, R. L. L., Schwartz, R,
Joshi, V., Singh, S., and Smith, N. A. Knowledge en-
hanced contextual word representations, 2019.

Petroni, F., Rocktaschel, T., Lewis, P., Bakhtin, A., Wu,
Y., Miller, A. H., and Riedel, S. Language models as knowledge bases?
arXiv preprint arXiv:1909.01066,
2019.

Radford, A., Narasimhan, K., Salimans, T., and Sutskever,
I. Improving language understanding with unsupervised learning.
Technical report, OpenAL, 2018.

Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and
Sutskever, I. Language models are unsupervised multitask learners.
OpenAI Blog, 2019.

Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S,
Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer.
arXiv preprint arXiv: 1910.10683,
2019.

Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. Squad:
100,000+ questions for machine comprehension of text.
In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383-
2392, 2016.
            </text>
            

            <text>
            Rajpurkar, P., Jia, R., and Liang, P. Know what you donâ€™t know: Unanswerable questions for squad. arXiv preprint arXiv: 1806.03822, 2018.

Ram, P. and Gray, A. G. Maximum inner-product search using cone trees. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 931-939, 2012.

Roberts, A., Raffel, C., and Shazeer, N. How much knowledge can you pack into the parameters of a language model? arXiv preprint arXiv:TBD, 2020.

Robertson, S., Zaragoza, H., et al. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends in Information Retrieval, 3(4):333-389, 2009.

Sang, E. T. K. and De Meulder, F. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pp. 142-147, 2003.

Seo, M., Kembhavi, A., Farhadi, A., and Hajishirzi, H. Bidirectional attention flow for machine comprehension. In International Conference on Learning Representations,

Shen, F., Liu, W., Zhang, S., Yang, Y., and Tao Shen, H. Learning binary codes for maximum inner product search. In Proceedings of the IEEE International Conference on Computer Vision, pp. 4148-4156, 2015.

Shrivastava, A. and Li, P. Asymmetric Ish (alsh) for sub-linear time maximum inner product search (mips). In Advances in Neural Information Processing Systems, pp.

Sukhbaatar, S., Weston, J., Fergus, R., et al. End-to-end memory networks. In Advances in neural information processing systems, 2015.

Weston, J., Chopra, S., and Bordes, A. Memory networks. arXiv preprint arXiv: 1410.3916, 2014.
            </text>
            
