
            <text>
            language model pre-training algorithms with a learned _textual knowledge retrieval._ In contrast to models that store knowledge in their parameters, this approach _explicitly_ exposes the role of world knowledge by asking the model to decide what knowledge to retrieve and use during inference. Before making each prediction, the language model uses the retriever to retrieve documents^1 from a large corpus such as Wikipedia, and then attends over those documents to help inform its prediction. Learning this model end-to-end requires backpropagating through a retrieval step that considers an entire corpus of textual knowledge, as shown in Figure 1.


            </text>
            

            <text>
            The key intuition of REALM is to train the retriever using a _performance-based_ signal from unsupervised text: a retrieval that _improves_ the language model's perplexity is helpful and should be rewarded, while an uninformative retrieval should be penalized. For example, in Figure 1, if the model needs to fill the blank in "the ‚Äì at the top of the pyramid", the retriever should be rewarded for selecting a document containing "The pyramid on top allows for less material higher up the pyramid". We achieve this behavior by modeling our _retriever-then-predict_ approach as a latent variable language model and optimizing the marginal likelihood.


            </text>
            

            <text>
            Incorporating a large-scale neural retrieval module during pre-training constitutes a significant computational challenge, since the retriever must consider millions of candidate documents for each pre-training step, and we must backpropagate through its decisions. To address this, we structure the retriever such that the computation performed for each document can be cached and asynchronously updated, and selection of the best documents can be formulated as Maximum Inner Product Search (MIPS).


            </text>
            

            <text>
            Numerous prior works have demonstrated the benefit of adding a discrete retrieval step to neural networks (Miller et al., 2016; Chen et al., 2017), but did not apply the framework to language model pre-training and employed non-learned retrievers to handle large-scale document collections. In the language modeling literature, the k-Nearest Neighbour language Model (Khandelwal et al., 2019) (kNN-LM) retrieves similar LM examples to improve memorization. However, kNN-LM was not fine-tuned for downstream tasks, perhaps because it is unclear how to adapt the retrieval mechanism: a kNN can only use examples labeled for the target task‚Äìduring fine-tuning, this precludes LM examples, which contain the desired world knowledge. In contrast, REALLY's retriever is designed to transfer to other tasks, and the retrieval is just text, not a labeled example.


            </text>
            

            <text>
            We evaluate our approach by fine-tuning the models pre-trained with REALM on the task of Open-domain Question Answering (Open-QA), one of the most knowledge-intensive tasks in natural language processing
            </text>
            

            <text>
            ing. We evaluate on three popular Open-QA benchmarks (NaturalQuestions-Open, WebQuestions, and CURat-EDRE) and compare to state-of-the-art Open-QA models, including both extremely large models that store knowledge implicitly (such as T5) as well as previous approaches that also use a knowledge retriever to access external knowledge, but implement retrieval in a more heuristic fashion (Lee et al., 2019; Min et al., 2019a; Asai et al., 2019). REALM achieves new state-of-the-art results on all three benchmarks, significantly outperforming all previous systems by 4-16


            </text>
            

            <title>
            2. Background
            </title>
            

            <text>
            

**Language model pre-training** The goal of language model pre-training is to learn useful representations of language, usually from unlabeled text corpora. The resulting pre-trained model can then be further trained (_fine-tuned_) for a downstream task of primary interest (in our case, OpenQA), often leading to better generalization than training from scratch (Dai     Le, 2015; Radford et al., 2019).


            </text>
            

            <text>
            We focus on the _masked language model^2_ (MLM) variant of pre-training popularized by BERT (Devlin et al., 2018). In its basic form, an MLM is trained to predict the missing tokens in an input text passage. Given an unlabeled pre-training corpus ùí≥ (e.g., Wikipedia text), a training example (x,y) can be generated by randomly masking tokens in a sampled piece of text (e.g., x= "The [MASK] is the currency [MASK] the UK"; y= ("pound", "of")). The model uses its representation of the masked input x to predict the token that should go in each mask. A good MLM must learn to encode syntactic and semantic information (e.g., to predict "of") as well as some world knowledge (e.g., to predict "pound").


            </text>
            

            <text>
            

**Open-domain question answering (Open-QA)** To measure a model's ability to incorporate world knowledge, we need a downstream task where world knowledge is critical. Perhaps one of the most knowledge-intensive tasks in natural language processing is open-domain question answering (Open-QA): given a question x such as "what is the currency of the UK?", a model must output the correct answer string y, "pound". The "open" part of Open-QA refers to the fact that the model does _not_ receive a pre-identified document that is known to contain the answer, unlike traditional reading comprehension (RC) tasks
            </text>
            

            <text>
            ^1We use the term "document" loosely to refer to a passage from the knowledge corpus, not necessarily a whole article.

^2Strictly speaking, MLM is not a standard language model, since it does not define a distribution over the entire sequence of tokens. In the paper we sometimes abuse the term "language model" slightly to make the phrase shorter.


            </text>
            
