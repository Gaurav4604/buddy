
            <text>
            language modeling (MLM) loss as in Devlin et al. (2018):
            </text>
            

            <formula>
            
    p(y | z,x)=∏_j=1^J_xp(y_j | z,x)

            </formula>
            

            <formula>
            
    p(y | z, x) = ∏_j=1^J_x p(y_j | z, x)



    p(y_j | z, x) ∝exp(w_j^⊤BERT_MASK(j)(join_BERT(x, z_body)))

            </formula>
            

            <text>
            
    p(y_j| z,x)∝exp(w_j^⊤BERT_MASK(j)(join_BERT(x,z_body)))

            </text>
            

            <formula>
            
    p(y_j| z,x)∝exp(w_j^⊤BERT_MASK(j)(join_BERT(x,z_body)))

            </formula>
            

            <text>
            where BERT_MASK(j) denotes the Transformer output vector corresponding to the j^th masked token, J_x is the total number of [MASK] tokens in x, and w_j is a learned word embedding for token y_j.


            </text>
            

            <text>
            For Open-QA fine-tuning, we wish to produce the answer string y. Following previous reading comprehension work (Rajpurkar et al., 2016; Seo et al., 2016; Lee et al., 2016; Clark     Gardner, 2017), we will assume that the answer y can be found as a contiguous sequence of tokens in some document z. Let S(z,y) be the set of spans matching y in z. Then we can define p(y | z,x) as:
            </text>
            

            <formula>
            
    p(y|z, x) ∝∑_s ∈ S(z, y)exp(MLP([h_START(s); h_END(s)])), 
     h_START(s) = BERT_START(s)(joint_BERT(x, z_body)), 
     h_END(s) = BERT_END(s)(joint_BERT(x, z_body))

            </formula>
            

            <text>
            where BERT_START(s) and BERT_END(s) denote the Transformer output vectors corresponding to the start and end tokens of span s, respectively, while MLP denotes a feed-forward neural network. We will let ϕ denote all parameters associated with the knowledge-augmented encoder.


            </text>
            

            <title>
            
    3.3. Translating

            </title>
            

            <text>
            For both pre-training and fine-tuning, we train by maximizing the log-likelihood log p(y | x) of the correct output y. Since both the knowledge retriever and knowledge-augmented encoder are differentiable neural networks, we can compute the gradient of log p(y | x) (defined in Equation 1) with respect to the model parameters θ and ϕ, and optimize using stochastic gradient descent.


            </text>
            

        <image>
        <path>
        outputs/general/images/chapter_0/page_3_11.jpg
        </path>
        <description>
        A diagram illustrating a two-step process for generating an answer to a question using neural knowledge retrievers and encoders. The left side shows unlabeled text with placeholders, while the right side displays pre-training corpus data and supervised learning input.
        </description>
        </image>
        

            <caption>
            
* The overall framework of REAMM. **Left**: _Unsupervised pre-training._ The knowledge retrieval and knowledge-augmented encoder are jointly pre-trained on the unsupervised language modeling task. **Right**: _Superviewed five-tuning._ After the parameters of the retrieval (θ) and encoder (ϕ) have been pre-trained, they are then fine-tuned on a task of primary interest, using supervised examples.


            </caption>
            

            <text>
            The key computational challenge is that the marginal probability p(y | x)=∑_z∈𝒵p(y | x,z) p(z | x) involves a summation over all documents z in the knowledge corpus 𝒵. We approximate this by instead summing over the top k documents with highest probability under p(z | x)–this is reasonable if most documents have near zero probability.


            </text>
            

            <text>
            Even with this approximation, we still need an efficient way to find the top k documents. Note that the ordering of documents under p(z | x) is the same as under the relevance score f(x,z)=_(x)^⊤_(z), which is an inner product. Thus, we can employ Maximum Inner Product Search (MIPS) algorithms to find the approximate top k documents, using running time and storage space that scale sub-linearly with the number of documents (Ram     Gray, 2012; Shrivastava     Li, 2014; Shen et al., 2015).


            </text>
            

            <text>
            To employ MIPS, we must pre-compute 𝖤𝗆𝖻𝖾𝖽_𝖽𝗈𝖼(z) for every z∈𝒵 and construct an efficient search index over these embeddings. However, this data structure will no longer be consistent with p(z | x) if the parameters θ of 𝖤𝗆𝖻𝖾𝖽_𝖽𝗈𝖼 are later updated. Hence, the search index goes "state" after every gradient update on θ.


            </text>
            

            <text>
            Our solution is to "refresh" the index by asynchronously re-embedding and re-indexing all documents every several hundred training steps. The MIPS index is slightly stable between refreshes, but note that it is _only_ used to select the top k documents. We recompute p(z | x) and its gradient, using the fresh θ, for these top k documents after retrieving them. In Section 4.5, we empirically demonstrate that this procedure results in stable optimization, provided that refreshes happen at a sufficiently frequent rate.


            </text>
            

            <text>
            

**Implementing asynchronous MIPS refreshes** We asynchronously refresh the MIPS index by running two jobs in parallel: a primary _trainer_ job, which performs gradient updates on the parameters, and a secondary _index builder_ job, which embeds and indexes the documents. As shown
            </text>
            
