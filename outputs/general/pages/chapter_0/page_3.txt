
            <text>
            such as SQuAD (Rajpurkar et al., 2016; 2018). While RC models comprehend a single document, Open-QA models must retain knowledge from millions of documents, since a question could be about any of them.


            </text>
            

            <text>
            We focus on Open-QA systems that utilize a textual knowledge corpus Z as the knowledge source. Many of these systems employ a retrieval-based approach: given a question x, retrieve potentially relevant documents z from the corpus Z, and then extract an answer y from the documents (Brill et al., 2002; Chen et al., 2017; Lee et al., 2019). Our approach, REALM, is inspired by this paradigm and extends it to language model pre-training. Alternatively, some recent work has proposed generation-based systems that apply a sequence-to-sequence model on x to directly generate y token-by-token (Lewis et al., 2019; Raffel et al., 2019). We will compare against state-of-the-art systems from both paradigms in our experiments.
            </text>
            

            <title>
            
    3.4¬†Approach

            </title>
            

            <text>
            We start by formalizing REALM's pre-training and fine-tuning tasks as a _retrieve-then-predict_ generative process in Section 3.1. Then in Section 3.2, we describe the model architectures for each component of that process. In Section 3.3, we show how to implement REALM pre-training and fine-tuning by maximizing the likelihood of REALM's generative process. En route, we address important computational challenges, explain why training works, and also discuss strategies for injecting useful inductive biases. The overall framework is illustrated in Figure 2.


            </text>
            

            <title>
            

## 3.1. REALLY's generative process
            </title>
            

            <text>
            For both pre-training and fine-tuning, REALM takes some input x and learns a distribution p(y | x) over possible outputs y. For pre-training, the task is masked language modeling: x is a sentence from a pre-training corpus ùí≥ with some tokens masked out, and the model must predict the value of those missing tokens, y. For fine-tuning, the task is Open-QA: x is a question, and y is the answer.


            </text>
            

            <text>
            REALLY decomposes p(y | x) into two steps: _retireve_, then _predict_. Given an input x, we first retrieve possibly helpful documents z from a knowledge corpus ùíµ. We model this as a sample from the distribution p(z | x). Then, we condition on both the retrieved z and the original input x to generate the output y‚Äìmodeled as p(y | z,x). To obtain the overall likelihood of generating y, we treat z as a latent variable and marginalize over all possible documents z, yielding 
            </text>
            

            <formula>
            
    p(y | x)=‚àë_z‚ààùíµp(y | z,x) p(z | x).

            </formula>
            

            <text>
            
    (ùê•)

            </text>
            

            <title>
            

## 3.2. Model architecture
            </title>
            

            <text>
            We now describe the two key components: the **neutral knowledge retriever**, which models p(z | x), and the knowledge-augmented encoder, which models p(y | z,x).


            </text>
            

            <text>
            

## Knowledge Retriever

The retriever is defined using a dense inner product model:
            </text>
            

            <formula>
            
    p(z|x) = exp f(x, z)/‚àë_z'exp f(x, z'), 
     f(x, z) = Embed_input(x)^‚ä§Embed_doc(z)

            </formula>
            

            <text>
            where Embed_input and Embed_doc are embedding functions that map x and z respectively to d-dimensional vectors. The _relevance score_f(x,z) between x and z is defined as the inner product of the vector embeddings. The retrieval distribution is the softmax over all relevance scores.


            </text>
            

            <text>
            We implement the embedding functions using BERT-style Transformers (Devlin et al., 2018). Following standard practices, we join spans of text by applying wordpiece tokenization, separating them with [SEP] tokens, prefixing a [CLS] token, and appending a final [SEP] token.


            </text>
            

            <formula>
            [                      join_BERT(x) = [ CLS ] x [ SEP ]; join_BERT(x_1, x_2) = [ CLS ] x_1 [ SEP ] x_2 [ SEP ] ]
            </formula>
            

            <text>
            As in Devlin et al. (2018), we pass this into a Transformer, which produces one vector for each token, including the vector corresponding to [CLS] which is used as a "pooled" representation of the sequence (denoted BERT_CLS). Finally, we perform a linear projection to reduce the dimensionality of the vector, denoted as a projection matrix ùêñ:
            </text>
            

            <formula>
            
    [          Embed_input(x)=ùêñ_inputBERT_CLS(j oin_BERT(x)); Embed_doc(z)=ùêñ_docBERT_CLS(j oin_BERT(z_title,z_body)) ]

            </formula>
            

            <text>
            where z_ title is the document's title and z_ body is its body. We let Œ∏ denote all parameters associated with the retriever, which include the Transformer and projection matrices.


            </text>
            

            <text>
            Knowledge-Augmented EncoderGiven an input x and a retrieved document z, the knowledge-augmented encoder defines p(y | z,x). We join x and z into a single sequence that we feed into a Transformer (distinct from the one used in the retrieve). This allows us to perform rich cross-attention between x and z before predicting y. See Figure 1 for a concrete example.


            </text>
            

            <text>
            At this stage, the architectures for pre-training and fine-tuning differ slightly. For the masked language model pre-training task, we must predict the original value of each token in x. To do so, we use the same masked
            </text>
            
